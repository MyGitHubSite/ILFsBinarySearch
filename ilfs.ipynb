{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 126x Faster Increased Limit Factor Calculations Using Binary Search\n",
    "\n",
    "## Background\n",
    "\n",
    "Actuaries calculate Increased Limit Factors (ILFs) to price various types of non-property insurance policies. With the increasing sizes of databases available to Actuaries, standard tools built in Excel start to slow to a crawl and many Actuaries have turned to R and Python to do their heavy lifting.\n",
    "\n",
    "This notebook shows two approaches to calculating ILFs given a database of trended and developed losses, the first being an intuitive but \"naive\" approach, the other being less intuitive but magnitudes faster for large datasets.\n",
    "\n",
    "While a deep understanding of ILFs is not needed to adopt the concepts in this notebook to other Actuarial problems, for those who are interested you can read about [ILFs here](https://www.casact.org/library/studynotes/palmer.pdf). Here we will focus on the main problem of calculating Limited Losses at various breakpoints and make simplifying assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Down the Problem\n",
    "\n",
    "Our goal is to calculate indemnity only ILFs using the loss listing we've been provided with. The ILF at limit L is calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "ILF(L) = \\frac{Limited\\,Average\\,Severity\\,(L)}{Limited\\,Average\\,Severity\\,(B)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The limited average severity is calculated simply by capping each of the losses to the corresponding limit, and taking the average of all the capped losses. For example, if we have three losses 54,000, 145,000 and 1,100,000, calculating the Limited Average Severity at 100,000 and 1,000,000 is as follows:\n",
    "\n",
    "$$\n",
    "LAS(100,000) = \\frac{54,000 + 100,000 + 100,000}{3} = 84,666\n",
    "$$\n",
    "\n",
    "| Loss # | Total Loss | Limited to 100,000 | Limited to 1,000,000 |\n",
    "|--------|------------|--------------------|----------------------|\n",
    "| 1      | 54,000     | 54,000             | 54,000               |\n",
    "| 2      | 145,000    | 100,000            | 145,000              |\n",
    "| 3      | 1,100,000  | 100,000            | 1,000,000            |\n",
    "| LAS    | 433,000    | 84,666             | 399,666              |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Data Simulation\n",
    "\n",
    "Using the emperical data on page 6 [of the above paper](https://www.casact.org/library/studynotes/palmer.pdf), I've fit a LogNormal distribution in R and simulated 1,000,000 losses. I've stored them in a simple PostgresSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import psycopg2 #<- Postgres utilities\n",
    "\n",
    "# R Fit:\n",
    "# meanlog sdlog\n",
    "# 11.50090134 0.84695921\n",
    "\n",
    "mean = 11.50090134\n",
    "sd = 0.84695921\n",
    "\n",
    "x=[]\n",
    "\n",
    "for i in range(1000000):\n",
    "    x.append(np.random.lognormal(mean,sd))\n",
    "\n",
    "# Sample Code for Inserting data into PostgresSQL\n",
    "#for i in range(len(x)):\n",
    "#    cur.execute(\"INSERT INTO liabdata (policyid, loss) VALUES (%s, %s)\", (i, x[i]))\n",
    "\n",
    "# Plot source code: https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.random.lognormal.html\n",
    "y = np.linspace(1, 5000000,1000)\n",
    "pdf = (np.exp(-(np.log(y) - mean)**2 / (2 * sd**2)) / (y * mean * np.sqrt(2 * np.pi)))\n",
    "plt.plot( y, pdf, linewidth=2, color='r')\n",
    "plt.axis('tight')\n",
    "plt.title(\"PDF of Simulated Losses\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Declaration\n",
    "\n",
    "For this exercise, we're going to only use the first 100,000 records as otherwise you'll be waiting all day for the first algorithm to run. We'll soon see that the alternative method can process all 1M records with great efficiency.\n",
    "\n",
    "I've provided code below to show how the data could be pulled from a PostgresSQL - for purposes of this Notebook we'll use the freshly simulated losses above. We'll also declare limits at which we want to calculate our ILFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Losses\n",
      "1     90823.847667\n",
      "2    184309.965162\n",
      "3     12055.954492\n",
      "4    161196.093325\n",
      "5     40252.833780\n",
      "6    196617.820361\n",
      "7     65287.683064\n",
      "8     78439.947662\n",
      "9    187492.154941\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Sample Code for pulling data from PostgresSQL\n",
    "#conn = psycopg2.connect(\"dbname=actuar user=ryanwilkins\")\n",
    "#sql = \"SELECT * FROM liabdata\"\n",
    "#data = pd.read_sql_query(sql, conn)\n",
    "#losses = data[\"loss\"][0:100000].copy()\n",
    "\n",
    "losses = pd.Series(x)[0:100000].copy()\n",
    "\n",
    "# Define our Limits\n",
    "# Subset: Each 100K band from 100k to 1M\n",
    "limits = [(i+1)*100000 for i in range(10)]\n",
    "# All: Each 100K band from 100k to 25M\n",
    "limits_all = [(i+1)*100000 for i in range(250)]\n",
    "\n",
    "print(\"Sample Losses\")\n",
    "print(losses[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Naive Algorithm\n",
    "\n",
    "When programmers talk about Naive algorithms, they're not taking a knock at the intelligence of the person who wrote it, rather stating that the alogorithm itself is simple. This approach uses a brute force approach, comparing each loss to each limit:\n",
    "___\n",
    "\n",
    "<span style=\"color:blue; font-size:1.5em\">Naive Algorithm for Calculating Limited Average Severity</span>\n",
    "\n",
    "* For Each Limit of Interest, $L$\n",
    "    * Loop through each loss `l`    \n",
    "        * Create a storage array, `temp`    \n",
    "        * if `l` < $L$:                \n",
    "            * Append `l` to `temp`                   \n",
    "        * else if `l` >= $L$:                \n",
    "            * Append $L$ to `temp`                        \n",
    "    * Take the average of `temp` and store in `Limited Losses` dictionary, at index $L$\n",
    "    * Repeat for next $L$\n",
    "* Output LL\n",
    "___\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "The block below shows this algorithm coded into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_limited_loss(limits, losses):\n",
    "    \n",
    "    # Store the average limited losses in a dict\n",
    "    LL = {}\n",
    "    \n",
    "    for i in range(len(limits)):\n",
    "        \n",
    "        # Storage Array\n",
    "        temp = [0] * len(losses)\n",
    "        curr_limit = limits[i]\n",
    "        \n",
    "        # Iterate through losses (n calls)\n",
    "        for j in range(len(losses)):\n",
    "            if curr_limit < losses[j]:\n",
    "                temp[j] = curr_limit\n",
    "            else:\n",
    "                temp[j] = losses[j]\n",
    "                \n",
    "        # Store average in result dictionary\n",
    "        LL[curr_limit] = np.mean(temp)\n",
    "\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Naive Algorithm\n",
    "\n",
    "Using the 10 limits we defined above, let's run and output the LAS results as well as the time it takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Limit            LAS\n",
      "0   100000   77889.543239\n",
      "1   200000  110010.325722\n",
      "2   300000  123958.230706\n",
      "3   400000  130830.905213\n",
      "4   500000  134527.218097\n",
      "5   600000  136688.077623\n",
      "6   700000  138034.149015\n",
      "7   800000  138884.671085\n",
      "8   900000  139442.343075\n",
      "9  1000000  139824.791256\n",
      "Naive Limited Losses Running Time: 19.038751125335693\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "naive_ll = naive_limited_loss(limits, losses)\n",
    "t2 = time()\n",
    "\n",
    "naivedf = pd.DataFrame(naive_ll.items())\n",
    "naivedf.columns = [\"Limit\", \"LAS\"]\n",
    "\n",
    "print(naivedf)\n",
    "\n",
    "print(\"Naive Limited Losses Running Time: {}\".format(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing this, my machine displays a running time of <b> 18.73 </b> seconds. If you're thinking to yourself \"That doesn't sound that long!\" just remember that we're only looking at a *small* fraction of the available data, and to Actuaries working at large national carriers, that might not even be a fraction of what they have access too! Let's do a rough calculation on what we could expect the running time of the full dataset with all 250 limits to be:\n",
    "\n",
    "$$\n",
    "Running\\;Time\\;Estimate: 18.73\\,sec * \\frac{250}{10} \\, Limits * \\frac{1M}{100K} \\, Records = 4,682\\,seconds = 78 \\;Minutes! \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Runtime of the Naive Algorithm\n",
    "\n",
    "Clearly, this algorithm is not efficient for very large datasets. Although it may give you an excuse to take a longer lunch break, if you're trying to run this on a frequent basis it will get tiresome quickly.\n",
    "\n",
    "Why is it so slow? The answer lies in looking at it's [Big-O](https://en.wikipedia.org/wiki/Big_O_notation) Runtime. Don't worry if you're not familiar with Big-O, we can still examine the algorithm and figure out the flaw.\n",
    "\n",
    "The key is in the nested for loop in the alogrithm. Take another look at this snippet of code. Roughly, how many calculations do we have to do?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each Limit is one call - len(limits) total\n",
    "for i in range(len(limits)):\n",
    "    ...\n",
    "    # Iterate through losses - len(losses) total\n",
    "    for j in range(len(losses)):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our small example, we're doing $len(limits) * len(losses) = 10 * 100k = 1,000,000$ calculations! The work done in this algorithm scales with the number of records, and is multiplied by a constant for the number of limits we example, and thus runs on $\\mathcal{O}(kn)$ time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup For The Efficient Algorithm\n",
    "\n",
    "The key step in the efficient setup involves first **sorting** the losses. Sorting is a fundamental concept in computer science, and there are a wide range of algorithms with different runtimes. Our data is straightforward enough that we can use [QuickSort](https://en.wikipedia.org/wiki/Quicksort), which luckily `pandas` has built right in.\n",
    "\n",
    "Note that when comparing our new algorithm to the old, we have to be fair and include the time it takes to sort the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuickSort Running Time: 0.02524590492248535\n"
     ]
    }
   ],
   "source": [
    "t3 = time()\n",
    "\n",
    "losses.sort_values(kind=\"quicksort\", inplace=True)\n",
    "losses = losses.reset_index(drop=True)\n",
    "\n",
    "t4 = time()\n",
    "\n",
    "print(\"QuickSort Running Time: {}\".format(t4-t3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Search \n",
    "\n",
    "The new algorithm will be based around the [Binary Search Algorithm](https://en.wikipedia.org/wiki/Binary_search_algorithm). The BSA will return the `index` of a search value `x` from a **sorted** array `arr`, with at most $\\log{}(n)$ (to be precise - $\\log{}_{2}(n)$) calls. How fast is $\\log{}(n)$? Let's examine the number of calls to the sort function needed in the worst case, versus [Linear Search](https://en.wikipedia.org/wiki/Linear_search) (searching left to right):\n",
    "\n",
    "| Number of Records | Linear Search O(n) | Binary Search O(log n) |\n",
    "|-------------------|--------------------|------------------------|\n",
    "| 100               | 100                | 7                      |\n",
    "| 1,000             | 1,000              | 10                     |\n",
    "| 100,000           | 100,000            | 17                     |\n",
    "| 1,000,000         | 1,000,000          | 20                     |\n",
    "\n",
    "Here I have written a custom Binary Search function. Note there are many available pre-made options out there, however because this is the focal pont of our analysis I want to explicitly show how it works. Optimizing this function for all the corner cases (i.e. loss list of size 0) is an exercise left up to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_loss_search(losses, x, l ,r):\n",
    "    if l<r:\n",
    "        m = (l+r)//2\n",
    "        if (losses[m] <= x) and (losses[m+1] >= x):\n",
    "            # Return the index to the closest matching loss\n",
    "            return m\n",
    "        if losses[m] < x:\n",
    "            return bin_loss_search(losses, x, m, r)\n",
    "        else:\n",
    "            return bin_loss_search(losses, x, l, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test of `bin_loss_search` should show it working as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of the first loss >= 1M is 99676. Surrounding losses: 999403.0750694824, 999565.091724513, 1000266.0079334187\n"
     ]
    }
   ],
   "source": [
    "# Find the position where 1M losses start:\n",
    "mil_loss_index = bin_loss_search(losses, 1000000, 0 , len(losses)-1)\n",
    "print(\"The index of the first loss >= 1M is {}. Surrounding losses: {}, {}, {}\".format(mil_loss_index,\n",
    "                                                                                       losses[mil_loss_index-1],\n",
    "                                                                                       losses[mil_loss_index],\n",
    "                                                                                       losses[mil_loss_index+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining The Efficient Algorithm\n",
    "\n",
    "Before we write the algorithm, take a second to consider what we know about our data so far:\n",
    "\n",
    "1. The `losses` are sorted from smallest to largest\n",
    "2. `bin_loss_search` return the index for the first loss equal to or greater than the input\n",
    "3. All losses after the `bin_loss_search` index will be greater than the input \n",
    "\n",
    "___\n",
    "\n",
    "<span style=\"color:blue; font-size:1.5em\">Efficient Algorithm for Calculating Limited Average Severity</span>\n",
    "\n",
    "1. Sort `losses`\n",
    "2. With Sorted `losses`:\n",
    "    * For Each Limit, $L$:\n",
    "        * Using `bin_loss_search` find index $i_{L}$ in `losses` where `losses[i_L]` $<= L <=$  `losses[i_L + 1]`\n",
    "        * Create `temp` of length `len(losses)`\n",
    "        * Set `temp[0:i_l]` = `losses[0:i_l]`\n",
    "        * Set `temp[i_l:len(losses)]` = $L$\n",
    "        * Take the average of `temp` and store in `LL`\n",
    "3. Return `LL`\n",
    "    \n",
    "___\n",
    "\n",
    "In code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_limited_loss(limits, losses):\n",
    "    LL = {}\n",
    "\n",
    "    for i in range(len(limits)):\n",
    "        \n",
    "        curr_limit = limits[i]\n",
    "        curr_index = bin_loss_search(losses, curr_limit, 0, len(losses)-1)\n",
    "        temp = [0] * len(losses)\n",
    "        temp[0:curr_index] = losses[0:curr_index]\n",
    "        temp[curr_index:len(losses)] = [curr_limit] * (len(losses)-curr_index)\n",
    "\n",
    "        LL[curr_limit] = np.mean(temp)\n",
    "\n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference? By using the fact that we know the `losses` are now in order, we can totally avoid looping through the whole dataset for each limit, and in fact the only computation needed for each limit involves setting up the `temp` array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Efficient Algorithm\n",
    "\n",
    "Using the same input as above, let's run `quick_limited_loss` and see if the output matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Limit            LAS\n",
      "0   100000   77889.543244\n",
      "1   200000  110010.325794\n",
      "2   300000  123958.230950\n",
      "3   400000  130830.905407\n",
      "4   500000  134527.219146\n",
      "5   600000  136688.080328\n",
      "6   700000  138034.149359\n",
      "7   800000  138884.671459\n",
      "8   900000  139442.347283\n",
      "9  1000000  139824.795605\n",
      "Quick Limited Loss Run Time: 0.2436990737915039\n",
      "Including Sort Run Time: 0.26894497871398926\n"
     ]
    }
   ],
   "source": [
    "t5 = time()\n",
    "quick_ll = quick_limited_loss(limits, losses)\n",
    "t6 = time()\n",
    "\n",
    "quickdf = pd.DataFrame(quick_ll.items())\n",
    "quickdf.columns = [\"Limit\", \"LAS\"]\n",
    "\n",
    "print(quickdf)\n",
    "\n",
    "print(\"Quick Limited Loss Run Time: {}\".format(t6-t5))\n",
    "print(\"Including Sort Run Time: {}\".format(t4-t3+t6-t5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtime of this algorithm on my machine, including the QuickSort, is **0.175 seconds**! Nice! \n",
    "\n",
    "Using the same exercise as before, let's estimate the runtime for the whole dataset and all limits. Noting that the 10 limits took **0.1576 seconds**, and ignoring the marginal extra time to sort 900K more records (thanks QuickSort!), a rough runtime estimate is:\n",
    "\n",
    "$$\n",
    "Running\\;Time\\;Estimate: 0.1576\\,sec * \\frac{250}{10} \\, Limits * \\frac{1M}{100K} \\, Records = 39.42 \\; seconds\n",
    "$$\n",
    "\n",
    "Actually running the algorithm on my machine shows that our estimate is pretty accurate:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quick_ll = quick_limited_loss(limits_all, losses)\n",
    "\n",
    "Quick Limited Loss Time: 41.98586583137512\n",
    "Including Sort Time: 42.1377968788147"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Runtime of the Efficient Algorithm\n",
    "\n",
    "Why is this new algorithm so much faster? As we've already discussed, this new algorithm only needs to create the `temp` array, and doesn't need to use a `for` loops to make `len(losses)` comparisons for each $L$. \n",
    "\n",
    "Let's look at the running time each step of the algorithm:\n",
    "\n",
    "* QuickSort: $\\mathcal{O}(n\\log{}n)$ (Average): Though worst case QuickSort runs in $\\mathcal{O}(n^2)$ time, for all practical applications the average speed is applicable. Our example above has shown that the `pandas` implementation of QuickSort is fast enough to sort 1M records in negligible time.\n",
    "\n",
    "* BinSearch: $\\mathcal{O}(\\log{}n)$: As discussed, the numbers of calls to BinSearch are at most ~17 for our 100,000 records.\n",
    "\n",
    "* Limits Loop: $\\mathcal{O}(k\\log{}n)$: For each limit in the table, we do a binary search, and an additional amount of work that scales with n to provision `temp`, but it negligable when compared to $k\\log{}n$.\n",
    "\n",
    "The smart reader will notice that for small values of `n` and `k`, it's possible for the Naive algorithm to outperform the Efficient algorithm. However, for any study with numbers that small, Excel would probably suffice.\n",
    "\n",
    "How much faster is the Efficient algorithm? Let's take a look at our example of 10 limits and 100,000 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Method Total Time:19.038751125335693\n",
      "Sorted Method Total Time: 0.26894497871398926\n",
      "Naive Method Time Relative to Sorted: 70.79050598517601\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Method Total Time:{}\".format(t2-t1))\n",
    "print(\"Sorted Method Total Time: {}\".format(t4-t3+t6-t5))\n",
    "print(\"Naive Method Time Relative to Sorted: {}\".format((t2-t1)/(t4-t3+t6-t5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Slow\\;Algorithm: 20.18\\;Seconds\n",
    "$$\n",
    "$$\n",
    "Fast\\;Algorithm: 0.159\\;Seconds\n",
    "$$\n",
    "$$\n",
    "126\\; Times \\; Faster!\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rounding Out the Analysis and Extensions\n",
    "\n",
    "Let's calculate and output the actual ILFs to make our analysis complete. Using the Basic Limits Loss of 100,000 the ILFs can be found via a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Limit            LAS       ILF\n",
      "0   100000   77889.543244  1.000000\n",
      "1   200000  110010.325794  1.412389\n",
      "2   300000  123958.230950  1.591462\n",
      "3   400000  130830.905407  1.679698\n",
      "4   500000  134527.219146  1.727154\n",
      "5   600000  136688.080328  1.754896\n",
      "6   700000  138034.149359  1.772178\n",
      "7   800000  138884.671459  1.783098\n",
      "8   900000  139442.347283  1.790258\n",
      "9  1000000  139824.795605  1.795168\n"
     ]
    }
   ],
   "source": [
    "quickdf[\"ILF\"] = quickdf[\"LAS\"]/(quick_ll[100000])\n",
    "\n",
    "print(quickdf)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy!\n",
    "\n",
    "Now, our Efficient Algorithm still has room for improvement. For example, can you think of a faster way to calculate the numerator of the LAS? Do we actually need to fill out the rest of `temp` with $L$? Leave me a comment and let me know what other ways you can think to improve the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}